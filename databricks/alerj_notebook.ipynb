{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a38bbec6-4c83-4975-914d-76e1d89fefb3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "# spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.email\", \"bucket-bigquery@leafy-environs-409823.iam.gserviceaccount.com\")\n",
    "# spark.conf.set(\"spark.hadoop.fs.gs.project.id\", \"leafy-environs-409823\")\n",
    "# spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key\", dbutils.secrets.get(scope='gcp-bucket', key='databricks-bucket-key'))\n",
    "# spark.conf.set(\"spark.hadoop.fs.gs.auth.service.account.private.key.id\", dbutils.secrets.get(scope='gcp-bucket', key='databricks-bucket-key_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ff825c-bbf3-43d2-a98e-38239e141746",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "from pyspark.sql.functions import col, when, regexp_replace, lit\n",
    "from pyspark.sql.types import FloatType\n",
    "import TypeError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eba571e-7979-43dd-bcc8-90f2382b0577",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# GCP secret -> steps to install\n",
    "# databricks configure --token\n",
    "# databricks secrets create-scope gcp-bucket --initial-manage-principal users\n",
    "# databricks secrets put-secret gcp-bucket databricks-bucket-key\n",
    "# databricks secrets put-secret gcp-bucket databricks-bucket-key_id\n",
    "# Add config on Cluster Spark config https://docs.gcp.databricks.com/en/connect/storage/gcs.html\n",
    "\n",
    "print(dbutils.secrets.listScopes())\n",
    "print(dbutils.secrets.list('gcp-bucket'))\n",
    "\n",
    "dbutils.fs.ls(\"gs://bossa-bucket-coutj/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55e8bff2-63e5-45ad-a570-e3ba4958790a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#READING datasus raw data parquet\n",
    "\n",
    "dbutils.widgets.text(\"file_name\", \"\", \"Enter file_name\")\n",
    "file_name = dbutils.widgets.get('file_name') or '2016/folha_2016_1.parquet'\n",
    "# print(file_name)\n",
    "df = spark.read.parquet(f\"gs://bossa-bucket-coutj/raw/{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83508e43-b779-4d26-91ab-7d8dccabffa8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove \"-\"\n",
    "\n",
    "for column in df.columns:\n",
    "    # print(column)\n",
    "    df = df.withColumn(column, when( col(column)=='-', None).otherwise(col(column)))\n",
    "    \n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c6c055-f5ad-4e64-a10d-6f5a84f5bf3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fix currency format and cast string values to float\n",
    "numeric_columns = [col for col in df.columns if col not in ['nome', 'cargo', 'funcao']]\n",
    "\n",
    "for column in numeric_columns:\n",
    "    df = df.withColumn(\n",
    "                column, regexp_replace(col(column), \"\\.\", \"\")\n",
    "                ).withColumn(column, regexp_replace(column, \",\", \".\").cast(\"float\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "434a477b-5ec4-411d-8ebf-754bcb0f696d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define data date\n",
    "\n",
    "date_str = re.search(r\"(?<=folha_)(.*)(?=\\.parquet)\",file_name).group(0).split('_')\n",
    "date = datetime.strptime(f\"{date_str[0]}/{date_str[1]}/1\",\"%Y/%m/%d\")\n",
    "table_name = f\"{date_str[0]}_{date_str[1]}\"\n",
    "# Add date column\n",
    "df = df.withColumn('date',lit(date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4add3f19-a9c9-44f1-b0cd-b547b81a0494",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "out_table_schema = {\n",
    "    \"nome\": [\n",
    "        \"nome\"\n",
    "    ],\n",
    "    \"cargo\": [\n",
    "        \"cargo\"\n",
    "    ],\n",
    "    \"funcao\": [\n",
    "        \"funcao\"\n",
    "    ],\n",
    "    \"rendimento_do_funcionario\": [\n",
    "        \"rendfunc\",\n",
    "        \"rendfuncionario\",\n",
    "        \"rendimentos_do_funcionario\",\n",
    "        \"rendimento_func\",\n",
    "        \"rendimento_funcionario\",\n",
    "        \"vencimento\",\n",
    "        \"rendimento_do_funcionario\"\n",
    "    ],\n",
    "    \"comissao\": [\n",
    "        \"comissao\"\n",
    "    ],\n",
    "    \"representacao_grat_seguranca\": [\n",
    "        \"representacao__grat_seguranca\",\n",
    "        \"representacao_grat_segur\",\n",
    "        \"representacao_grat_segur_qualificacao\",\n",
    "        \"represent_grat_segur\",\n",
    "        \"representacao_grat_seguranca\",\n",
    "        \"representacao__grat_segur_grat_qualificacao_sfam\",\n",
    "        \"representacao_grat\",\n",
    "        \"representacao_qualificacao_grat_seguranca__s_fam\",\n",
    "        \"representacao_grat_seguranca_grat_qualificacao_s_fam\",\n",
    "        \"represent_gratseg\",\n",
    "        \"representacao_grat_segur_grat_qualificacao_sfam\"\n",
    "    ],\n",
    "    \"incorporado\": [\n",
    "        \"incorporado\",\n",
    "        \"eignucrorporado\"\n",
    "    ],\n",
    "    \"trienio\": [\n",
    "        \"trienio\"\n",
    "    ],\n",
    "    \"bolsa_reforco_escolar\": [\n",
    "        \"abono_de_permanencia\",\n",
    "        \"bolsa_reforco_escolar_abono_permanencia\",\n",
    "        \"bolsa_reforco_escolar\",\n",
    "        \"bolsa_reforco_escolar__abono_permanencia\"\n",
    "    ],\n",
    "    \"ferias\": [\n",
    "        \"ferias\"\n",
    "    ],\n",
    "    \"redutor\": [\n",
    "        \"redutor\"\n",
    "    ],\n",
    "    \"ipalerj_mensalidade\": [\n",
    "        \"ipalerj__mensalidade\",\n",
    "        \"ipalerj_mensalida_de\",\n",
    "        \"ipalerj_mensalidade\",\n",
    "        \"ipalerj_mens\"\n",
    "    ],\n",
    "    \"pensao_alimenticia\": [\n",
    "        \"pensao_alimenticia\"\n",
    "    ],\n",
    "    \"previdencia_inss\": [\n",
    "        \"previdencia_inss\",\n",
    "        \"previdencia\"\n",
    "    ],\n",
    "    \"imposto_de_renda\": [\n",
    "        \"ir\",\n",
    "        \"imposto_de_renda\",\n",
    "        \"imp_de_renda\"\n",
    "    ],\n",
    "    \"indenizatoria\": [\n",
    "        \"indenizatoria\"\n",
    "    ],\n",
    "    \"rendimento_liquido\": [\n",
    "        \"total_liquido\",\n",
    "        \"rendimento_liquido\"\n",
    "    ],\n",
    "    \"mes_referencia\": [\n",
    "        \"date\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Fix column names\n",
    "for column in df.columns:\n",
    "    found = False\n",
    "    for col_ref_name in out_table_schema:\n",
    "        if column in out_table_schema[col_ref_name]:\n",
    "            found = True\n",
    "            df = df.withColumnRenamed(column, col_ref_name)\n",
    "    if not found:\n",
    "        raise Exception(f\"Found columns not mapped on the schema: {column}\")\n",
    "\n",
    "# Adding empty non existing columns \n",
    "for schema_column in out_table_schema:\n",
    "    if schema_column not in df.columns:\n",
    "        df = df.withColumn(schema_column, lit(None))\n",
    "        df = df.withColumn(schema_column, col(schema_column).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b93839e-9cf9-477a-a43d-3f54d1e5e09b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove rows with empty names\n",
    "\n",
    "df = df.na.drop(subset=['nome'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c28dae1-7658-4811-afac-18e244367932",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0becf4-df7e-46dc-9f51-0180eee03187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "try:\n",
    "    bq_lines = (spark.read.format('bigquery')\n",
    "        .option('table', f\"leafy-environs-409823.alerj_ds.alerj_payslip\")\n",
    "        .option(\"parentProject\", 'leafy-environs-409823')\n",
    "        .load())\n",
    "    bq_lines.createOrReplaceTempView('bq_table_view')\n",
    "\n",
    "    sql_query = f\"\"\"\n",
    "        SELECT * FROM bq_table_view where \n",
    "            nome=\"{df.first()[\"nome\"]}\" AND\n",
    "            mes_referencia=\"{df.first()[\"mes_referencia\"]}\" AND\n",
    "            rendimento_liquido=\"{df.first()[\"rendimento_liquido\"]}\"\n",
    "        \"\"\"\n",
    "    existing_data = spark.sql(sql_query)\n",
    "except Exception as e:\n",
    "    if \"alerj_ds.alerj_payslip\" in str(e.java_exception):\n",
    "        empty_schema = StructType([])\n",
    "        existing_data = spark.createDataFrame([], schema=empty_schema)\n",
    "    else:\n",
    "        raise Exception(f\"Error Reading From BigQuery: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cdd04ea-b5b2-4492-b7e2-96c55797033b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write dataframe to Bigquery:\n",
    "if not len(existing_data.take(1)):\n",
    "    (df.write.format(\"bigquery\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"project\", 'leafy-environs-409823')\n",
    "        .option(\"parentProject\", 'leafy-environs-409823')\n",
    "        .option(\"temporaryGcsBucket\",\"bossa-bucket-coutj\")\n",
    "        .option(\"table\",f\"leafy-environs-409823.alerj_ds.alerj_payslip\")\n",
    "        .save())\n",
    "else:\n",
    "    print(\"Data Already Exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ae0cca-86f2-4aa0-9476-6ef75e3d40c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').parquet(f'gs://bossa-bucket-coutj/trusted/{file_name}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "alerj_notebook",
   "widgets": {
    "file_name": {
     "currentValue": "",
     "nuid": "29a7b234-cc1c-4f35-9bb4-09160fff663d",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Enter file_name",
      "name": "file_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
